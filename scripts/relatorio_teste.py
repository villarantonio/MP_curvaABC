# -*- coding: utf-8 -*-
"""
SCRIPT ROBUSTO: CURVA ABC COM AN√ÅLISE IA E SISTEMA DE RETENTATIVA
Gera relat√≥rio ABC com insights de tend√™ncia usando Google Gemini.
"""

from __future__ import annotations

import logging
import os
import sys
import json
import time
import random
from typing import Any, Optional
from pathlib import Path

import pandas as pd
import google.generativeai as genai
from google.api_core import exceptions as google_exceptions
from dotenv import load_dotenv

# Carrega vari√°veis do arquivo .env
load_dotenv()

# ==========================================
# 1. CONFIGURA√á√ïES E CONSTANTES
# ==========================================

# Configura√ß√£o de logging estruturado
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# Configura√ß√µes do arquivo - aceita argumento de linha de comando ou usa valor padr√£o
# Suporta tanto CSV quanto XLSX
NOME_ARQUIVO = sys.argv[1] if len(sys.argv) > 1 else "dados_vendas.xlsx"
PASTA_SAIDA = "docs/data"
ARQUIVO_SAIDA = os.path.join(PASTA_SAIDA, "analise_abc_final.json")
ARQUIVO_CACHE = os.path.join(PASTA_SAIDA, "cache_analises_ia.json")

# Colunas esperadas do CSV
COL_LOJA = 'FtoResumoVendaGeralItem[loja_id]'
COL_PRODUTO = 'FtoResumoVendaGeralItem[material_descr]'
COL_VALOR = 'FtoResumoVendaGeralItem[vl_total]'
COL_DATA = 'FtoResumoVendaGeralItem[dt_contabil]'

# Par√¢metros da Curva ABC
LIMITE_CLASSE_A = 80  # Percentual acumulado para classe A
LIMITE_CLASSE_B = 95  # Percentual acumulado para classe B

# Par√¢metros de processamento IA
TAMANHO_LOTE_IA = 15
PAUSA_ENTRE_LOTES = 2.0  # segundos entre lotes
MAX_TENTATIVAS_API = 5   # tentativas para erros gerais
MAX_TENTATIVAS_RATE_LIMIT = 8  # tentativas extras para rate limit
DELAY_BASE_RATE_LIMIT = 15  # segundos base para rate limit (reduzido para plano pago)
DELAY_ENTRE_CHAMADAS = 12.0  # segundos entre cada chamada √† API (plano pago tem rate limit maior)

# API Key - carrega de vari√°vel de ambiente (NUNCA commitar chaves no c√≥digo!)
API_KEY = os.environ.get('GEMINI_API_KEY', '')

# ==========================================
# 2. FUN√á√ïES AUXILIARES
# ==========================================

def limpar_valor_monetario(valor: Any) -> float:
    """
    Converte valor monet√°rio brasileiro (1.234,56) para float.

    Args:
        valor: Valor a ser convertido (string ou num√©rico)

    Returns:
        Valor como float, ou 0.0 em caso de erro
    """
    if pd.isna(valor):
        return 0.0

    if isinstance(valor, (int, float)):
        return float(valor)

    if isinstance(valor, str):
        try:
            # Remove pontos de milhar e troca v√≠rgula por ponto
            valor_limpo = valor.strip().replace('.', '').replace(',', '.')
            return float(valor_limpo)
        except ValueError:
            logger.warning(f"N√£o foi poss√≠vel converter valor: '{valor}'")
            return 0.0

    return 0.0


def classificar_abc(valor_acumulado: float) -> str:
    """
    Classifica item na curva ABC baseado no percentual acumulado.

    Args:
        valor_acumulado: Percentual acumulado de vendas

    Returns:
        Classe 'A', 'B' ou 'C'
    """
    if valor_acumulado <= LIMITE_CLASSE_A:
        return 'A'
    elif valor_acumulado <= LIMITE_CLASSE_B:
        return 'B'
    return 'C'


def validar_colunas_csv(df: pd.DataFrame) -> bool:
    """
    Valida se o DataFrame cont√©m todas as colunas necess√°rias.

    Args:
        df: DataFrame a ser validado

    Returns:
        True se v√°lido, False caso contr√°rio
    """
    colunas_requeridas = [COL_LOJA, COL_PRODUTO, COL_VALOR, COL_DATA]
    colunas_faltantes = [col for col in colunas_requeridas if col not in df.columns]

    if colunas_faltantes:
        logger.error(f"Colunas faltantes no CSV: {colunas_faltantes}")
        logger.info(f"Colunas dispon√≠veis: {list(df.columns)}")
        return False

    return True


# ==========================================
# 2.5. FUN√á√ïES DE CACHE
# ==========================================

def carregar_cache() -> dict:
    """
    Carrega o cache de an√°lises anteriores do arquivo JSON.

    Returns:
        Dicion√°rio com an√°lises em cache: {loja_id: {produto: analise}}
    """
    if not os.path.exists(ARQUIVO_CACHE):
        logger.info("Nenhum cache encontrado. Iniciando cache vazio.")
        return {}

    try:
        with open(ARQUIVO_CACHE, 'r', encoding='utf-8') as f:
            cache = json.load(f)
        logger.info(f"Cache carregado: {sum(len(v) for v in cache.values())} an√°lises de {len(cache)} lojas")
        return cache
    except (json.JSONDecodeError, IOError) as e:
        logger.warning(f"Erro ao carregar cache: {e}. Iniciando cache vazio.")
        return {}


def salvar_cache(cache: dict) -> bool:
    """
    Salva o cache de an√°lises no arquivo JSON.

    Args:
        cache: Dicion√°rio com an√°lises

    Returns:
        True se salvou com sucesso
    """
    try:
        Path(ARQUIVO_CACHE).parent.mkdir(parents=True, exist_ok=True)
        with open(ARQUIVO_CACHE, 'w', encoding='utf-8') as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)
        logger.info(f"Cache salvo: {sum(len(v) for v in cache.values())} an√°lises")
        return True
    except IOError as e:
        logger.error(f"Erro ao salvar cache: {e}")
        return False


def gerar_chave_produto(produto: str, classe: str) -> str:
    """
    Gera uma chave √∫nica para o produto no cache.
    A chave inclui a classe ABC para re-analisar se a classifica√ß√£o mudar.

    Args:
        produto: Nome do produto
        classe: Classifica√ß√£o ABC (A, B ou C)

    Returns:
        Chave √∫nica para o cache
    """
    return f"{produto}|{classe}"


def obter_analise_cache(cache: dict, id_loja: str, produto: str, classe: str) -> Optional[str]:
    """
    Busca an√°lise no cache.

    Args:
        cache: Dicion√°rio de cache
        id_loja: ID da loja
        produto: Nome do produto
        classe: Classifica√ß√£o ABC

    Returns:
        An√°lise do cache ou None se n√£o encontrada
    """
    loja_cache = cache.get(str(id_loja), {})
    chave = gerar_chave_produto(produto, classe)
    return loja_cache.get(chave)


def adicionar_ao_cache(cache: dict, id_loja: str, produto: str, classe: str, analise: str) -> None:
    """
    Adiciona uma an√°lise ao cache.

    Args:
        cache: Dicion√°rio de cache
        id_loja: ID da loja
        produto: Nome do produto
        classe: Classifica√ß√£o ABC
        analise: Texto da an√°lise
    """
    id_loja_str = str(id_loja)
    if id_loja_str not in cache:
        cache[id_loja_str] = {}

    chave = gerar_chave_produto(produto, classe)
    cache[id_loja_str][chave] = analise


# ==========================================
# 3. FUN√á√ïES DE INTEGRA√á√ÉO COM IA
# ==========================================

def configurar_ia() -> Optional[genai.GenerativeModel]:
    """
    Configura e retorna o modelo Gemini para an√°lise.

    Returns:
        Modelo configurado ou None se n√£o dispon√≠vel
    """
    if not API_KEY:
        logger.warning("API Key n√£o configurada. An√°lise IA ser√° pulada.")
        return None

    try:
        genai.configure(api_key=API_KEY)
        modelo = genai.GenerativeModel(
            model_name="gemini-2.0-flash-lite",  # Modelo com rate limits mais altos
            generation_config={
                "temperature": 0.2,
                "response_mime_type": "application/json"
            }
        )
        logger.info("Modelo Gemini 2.0 Flash Lite configurado com sucesso")
        return modelo
    except Exception as e:
        logger.error(f"Erro ao configurar modelo Gemini: {e}")
        return None


def analisar_lote_ia_robusto(
    modelo: genai.GenerativeModel,
    id_loja: Any,
    lote_itens: list[dict],
    tentativas_max: int = MAX_TENTATIVAS_API
) -> list[dict]:
    """
    Envia lote de itens para an√°lise IA com sistema de retentativa robusto.

    Implementa exponential backoff para lidar com erros tempor√°rios
    como RemoteDisconnected, Timeout, rate limiting, etc.

    Para rate limit (429), usa delays maiores e mais tentativas.

    Args:
        modelo: Modelo Gemini configurado
        id_loja: Identificador da loja
        lote_itens: Lista de itens para an√°lise
        tentativas_max: N√∫mero m√°ximo de tentativas

    Returns:
        Lista de an√°lises ou lista vazia em caso de falha
    """
    if not modelo:
        return []

    if not lote_itens:
        return []

    prompt = f"""# AN√ÅLISE DE PRODUTOS - CURVA ABC

Analise cada produto da Loja {id_loja} individualmente.

## REGRAS:
- Forne√ßa um diagn√≥stico DIRETO sobre cada produto
- N√ÉO fa√ßa compara√ß√µes entre produtos
- N√ÉO mencione tend√™ncias, varia√ß√µes ou percentuais
- Foque no POTENCIAL e A√á√ÉO PR√ÅTICA

## DADOS:
{json.dumps(lote_itens, ensure_ascii=False)}

## FORMATO DE RESPOSTA (JSON):
Retorne EXATAMENTE um array JSON:
[
  {{"produto": "NOME_EXATO", "analise": "Diagn√≥stico direto e a√ß√£o pr√°tica (m√°x 60 chars)"}}
]

## EXEMPLOS:
{{"produto": "PICANHA", "analise": "Produto estrela - manter destaque no card√°pio"}}
{{"produto": "SOPA", "analise": "Baixa procura - promover ou reduzir preparo"}}
"""

    tentativas_rate_limit = 0  # Contador separado para rate limit
    tentativa = 0

    while tentativa < tentativas_max or tentativas_rate_limit < MAX_TENTATIVAS_RATE_LIMIT:
        tentativa += 1
        try:
            # Pausa entre chamadas para evitar rate limit
            time.sleep(DELAY_ENTRE_CHAMADAS)

            resposta = modelo.generate_content(prompt)

            # Valida se h√° resposta
            if not resposta or not resposta.text:
                logger.warning(f"Resposta vazia da IA na tentativa {tentativa}")
                continue

            # Parse e valida√ß√£o do JSON
            resultado = json.loads(resposta.text)

            # Valida estrutura da resposta
            if not isinstance(resultado, list):
                logger.warning(f"Resposta IA n√£o √© lista: {type(resultado)}")
                return []

            return resultado

        except json.JSONDecodeError as e:
            logger.warning(f"Erro ao parsear JSON da IA: {e}")
            return []

        except google_exceptions.ResourceExhausted as e:
            tentativas_rate_limit += 1
            # Delay progressivo: 30s, 60s, 90s, 120s... (mais agressivo para rate limit)
            tempo_espera = DELAY_BASE_RATE_LIMIT * tentativas_rate_limit + random.uniform(0, 5)
            logger.warning(
                f"‚ö†Ô∏è Rate limit atingido! Tentativa {tentativas_rate_limit}/{MAX_TENTATIVAS_RATE_LIMIT}. "
                f"Aguardando {tempo_espera:.0f}s..."
            )
            time.sleep(tempo_espera)

            if tentativas_rate_limit >= MAX_TENTATIVAS_RATE_LIMIT:
                logger.error("‚ùå Rate limit persistente. Pulando este lote.")
                return []

            # N√£o incrementa tentativa normal para rate limit
            tentativa -= 1
            continue

        except (google_exceptions.ServiceUnavailable,
                google_exceptions.DeadlineExceeded,
                ConnectionError) as e:
            tempo_espera = (2 ** tentativa) + random.uniform(0, 1)
            logger.warning(f"Erro de conex√£o na tentativa {tentativa}/{tentativas_max}: {e}")

            if tentativa < tentativas_max:
                logger.info(f"Tentando novamente em {tempo_espera:.1f}s...")
                time.sleep(tempo_espera)
            else:
                logger.error("Falha definitiva neste lote ap√≥s todas as tentativas.")
                return []

        except Exception as e:
            logger.error(f"Erro inesperado na an√°lise IA: {type(e).__name__}: {e}")
            return []

    return []

# ==========================================
# 4. FUN√á√ïES DE PROCESSAMENTO DE DADOS
# ==========================================

def carregar_dados(caminho: str) -> Optional[pd.DataFrame]:
    """
    Carrega arquivo de dados (CSV ou XLSX) com tratamento autom√°tico de formato.

    Args:
        caminho: Caminho para o arquivo de dados (CSV ou XLSX)

    Returns:
        DataFrame carregado ou None em caso de erro
    """
    if not os.path.exists(caminho):
        logger.error(f"Arquivo n√£o encontrado: {caminho}")
        return None

    # Detecta formato pelo extens√£o
    extensao = caminho.lower().split('.')[-1]

    # Arquivos Excel (.xlsx)
    if extensao in ['xlsx', 'xls']:
        try:
            df = pd.read_excel(
                caminho,
                engine='openpyxl',
                dtype={COL_LOJA: str}  # Preserva ID como string
            )
            logger.info(f"Arquivo Excel carregado com sucesso")
            logger.info(f"Total de registros: {len(df)}")
            return df
        except Exception as e:
            logger.error(f"Erro ao carregar Excel: {e}")
            return None

    # Arquivos CSV
    encodings = ['latin1', 'utf-8', 'cp1252', 'iso-8859-1']

    for encoding in encodings:
        try:
            df = pd.read_csv(
                caminho,
                sep=';',
                encoding=encoding,
                on_bad_lines='skip',
                dtype={COL_LOJA: str}  # Preserva ID como string
            )
            logger.info(f"CSV carregado com sucesso (encoding: {encoding})")
            logger.info(f"Total de registros: {len(df)}")
            return df
        except UnicodeDecodeError:
            continue
        except Exception as e:
            logger.error(f"Erro ao carregar CSV com {encoding}: {e}")
            continue

    logger.error("N√£o foi poss√≠vel carregar o arquivo com nenhum formato suportado")
    return None


def preparar_dados(df: pd.DataFrame) -> Optional[pd.DataFrame]:
    """
    Limpa e prepara os dados para an√°lise ABC.

    Args:
        df: DataFrame bruto do CSV

    Returns:
        DataFrame preparado ou None em caso de erro
    """
    # Valida√ß√£o de colunas
    if not validar_colunas_csv(df):
        return None

    # Cria c√≥pia para n√£o modificar original
    df = df.copy()

    # Converte valores monet√°rios
    df['valor_limpo'] = df[COL_VALOR].apply(limpar_valor_monetario)

    # Remove registros com valor zero ou negativo
    registros_antes = len(df)
    df = df[df['valor_limpo'] > 0]
    registros_removidos = registros_antes - len(df)

    if registros_removidos > 0:
        logger.info(f"Removidos {registros_removidos} registros com valor <= 0")

    # Tratamento de data
    df['data_obj'] = pd.to_datetime(
        df[COL_DATA],
        dayfirst=True,
        errors='coerce'
    )

    # Verifica datas inv√°lidas
    datas_invalidas = df['data_obj'].isna().sum()
    if datas_invalidas > 0:
        logger.warning(f"Encontradas {datas_invalidas} datas inv√°lidas")

    # Cria per√≠odo m√™s/ano
    df['mes_ano'] = df['data_obj'].dt.to_period('M').astype(str)
    df = df.dropna(subset=['mes_ano'])

    # Padroniza nomes de produtos
    df[COL_PRODUTO] = (
        df[COL_PRODUTO]
        .astype(str)
        .str.strip()
        .str.upper()
        .str.replace(r'\s+', ' ', regex=True)  # Remove espa√ßos m√∫ltiplos
    )

    # Remove produtos vazios ou inv√°lidos
    df = df[df[COL_PRODUTO].notna() & (df[COL_PRODUTO] != '') & (df[COL_PRODUTO] != 'NAN')]

    logger.info(f"Dados preparados: {len(df)} registros v√°lidos")
    return df


def gerar_historico_vendas(df: pd.DataFrame) -> pd.DataFrame:
    """
    Gera hist√≥rico de vendas mensais agrupado por loja e produto.

    Args:
        df: DataFrame preparado

    Returns:
        DataFrame com totais e hist√≥rico por produto/loja
    """
    logger.info("Gerando hist√≥rico de vendas...")

    # Agrupa vendas por m√™s
    historico_mensal = (
        df.groupby([COL_LOJA, COL_PRODUTO, 'mes_ano'])['valor_limpo']
        .sum()
        .reset_index()
    )

    # Cria dicion√°rio de hist√≥rico para cada produto
    def criar_dict_historico(grupo: pd.DataFrame) -> dict:
        return dict(zip(grupo['mes_ano'], grupo['valor_limpo'].round(2)))

    df_historico = (
        historico_mensal
        .groupby([COL_LOJA, COL_PRODUTO], group_keys=False)
        .apply(criar_dict_historico, include_groups=False)
        .reset_index(name='historico_vendas')
    )

    # Calcula total de vendas por produto
    df_total = (
        df.groupby([COL_LOJA, COL_PRODUTO])['valor_limpo']
        .sum()
        .reset_index(name='total_vendas')
    )

    # Merge hist√≥rico com totais
    df_final = pd.merge(df_total, df_historico, on=[COL_LOJA, COL_PRODUTO])

    logger.info(f"Hist√≥rico gerado: {len(df_final)} produtos √∫nicos")
    return df_final


def processar_loja(
    df_loja: pd.DataFrame,
    id_loja: str,
    modelo: Optional[genai.GenerativeModel],
    cache: dict
) -> dict:
    """
    Processa dados de uma loja individual: curva ABC e an√°lise IA.

    Args:
        df_loja: DataFrame com dados da loja
        id_loja: Identificador da loja
        modelo: Modelo Gemini ou None
        cache: Dicion√°rio de cache com an√°lises anteriores

    Returns:
        Dicion√°rio com dados processados da loja
    """
    # Ordena por vendas (maior para menor)
    df_loja = df_loja.sort_values(by='total_vendas', ascending=False).copy()

    # Calcula curva ABC
    total_vendas = df_loja['total_vendas'].sum()

    if total_vendas == 0:
        logger.warning(f"Loja {id_loja} sem vendas v√°lidas")
        return {"id_loja": id_loja, "itens": []}

    df_loja['percentual'] = (df_loja['total_vendas'] / total_vendas * 100)
    df_loja['acumulado'] = df_loja['percentual'].cumsum()
    df_loja['classe'] = df_loja['acumulado'].apply(classificar_abc)

    # Monta lista de itens (usando to_dict para melhor performance)
    itens_loja = df_loja.apply(
        lambda row: {
            "produto": row[COL_PRODUTO],
            "valor_total": round(row['total_vendas'], 2),
            "classe": row['classe'],
            "historico": row['historico_vendas']
        },
        axis=1
    ).tolist()

    # An√°lise IA com lotes (usando cache)
    if modelo:
        itens_loja = processar_analise_ia(modelo, id_loja, itens_loja, cache)

    # Converte ID para int se poss√≠vel, sen√£o mant√©m string
    try:
        id_loja_final = int(id_loja)
    except (ValueError, TypeError):
        id_loja_final = str(id_loja)

    return {"id_loja": id_loja_final, "itens": itens_loja}


def processar_analise_ia(
    modelo: genai.GenerativeModel,
    id_loja: str,
    itens: list[dict],
    cache: dict
) -> list[dict]:
    """
    Processa an√°lise IA em lotes para todos os itens de uma loja.
    Usa cache para evitar chamadas duplicadas √† API Gemini.

    Args:
        modelo: Modelo Gemini configurado
        id_loja: Identificador da loja
        itens: Lista de itens para an√°lise
        cache: Dicion√°rio de cache com an√°lises anteriores

    Returns:
        Lista de itens com an√°lise IA adicionada
    """
    analises_finais = []
    itens_novos = []  # Itens que precisam de an√°lise IA
    itens_cache = []  # Itens que j√° t√™m an√°lise em cache

    # Separa itens em cache e novos
    for item in itens:
        analise_cache = obter_analise_cache(cache, id_loja, item['produto'], item['classe'])
        if analise_cache:
            item['analise_ia'] = analise_cache
            itens_cache.append(item)
        else:
            itens_novos.append(item)

    logger.info(f"  üì¶ Cache: {len(itens_cache)} produtos | üÜï Novos: {len(itens_novos)} produtos")

    # Adiciona itens do cache ao resultado final
    analises_finais.extend(itens_cache)

    # Se n√£o h√° itens novos, retorna direto
    if not itens_novos:
        logger.info(f"  ‚úÖ Todos os produtos j√° estavam em cache!")
        return analises_finais

    # Processa apenas os itens novos em lotes
    total_lotes = (len(itens_novos) + TAMANHO_LOTE_IA - 1) // TAMANHO_LOTE_IA

    for i, k in enumerate(range(0, len(itens_novos), TAMANHO_LOTE_IA)):
        lote = itens_novos[k:k + TAMANHO_LOTE_IA]

        logger.info(f"  Processando lote {i + 1}/{total_lotes} ({len(lote)} itens novos)")

        # Prepara dados m√≠nimos para IA
        lote_ia = [
            {
                'produto': item['produto'],
                'classe': item['classe'],
                'historico': item['historico']
            }
            for item in lote
        ]

        # Chamada √† IA com retentativa
        resultado_ia = analisar_lote_ia_robusto(modelo, id_loja, lote_ia)

        # Mapeia resultados
        dict_analises = {
            item.get('produto', ''): item.get('analise', '')
            for item in resultado_ia
            if isinstance(item, dict)
        }

        # Adiciona an√°lise a cada item e atualiza cache
        for item in lote:
            analise = dict_analises.get(item['produto'], "An√°lise indispon√≠vel")
            item['analise_ia'] = analise

            # Adiciona ao cache (exceto an√°lises indispon√≠veis)
            if analise != "An√°lise indispon√≠vel":
                adicionar_ao_cache(cache, id_loja, item['produto'], item['classe'], analise)

            analises_finais.append(item)

        # Pausa entre lotes para evitar rate limiting
        if k + TAMANHO_LOTE_IA < len(itens_novos):
            time.sleep(PAUSA_ENTRE_LOTES)

    return analises_finais


def salvar_resultado(resultado: list[dict], caminho: str) -> bool:
    """
    Salva resultado em arquivo JSON.

    Args:
        resultado: Lista de resultados por loja
        caminho: Caminho do arquivo de sa√≠da

    Returns:
        True se salvou com sucesso
    """
    try:
        # Cria diret√≥rio se n√£o existir
        Path(caminho).parent.mkdir(parents=True, exist_ok=True)

        with open(caminho, 'w', encoding='utf-8') as f:
            json.dump(
                resultado,
                f,
                indent=2,
                ensure_ascii=False,
                default=str
            )

        logger.info(f"Resultado salvo em: {caminho}")
        return True

    except IOError as e:
        logger.error(f"Erro ao salvar arquivo: {e}")
        return False


# ==========================================
# 5. FUN√á√ÉO PRINCIPAL
# ==========================================

def main() -> None:
    """
    Fun√ß√£o principal: orquestra o processamento completo.
    """
    logger.info("=" * 50)
    logger.info("INICIANDO AN√ÅLISE CURVA ABC COM IA")
    logger.info("=" * 50)

    # 1. Carregar dados
    df = carregar_dados(NOME_ARQUIVO)
    if df is None:
        return

    # 2. Preparar dados
    df = preparar_dados(df)
    if df is None:
        return

    # 3. Gerar hist√≥rico
    df_processado = gerar_historico_vendas(df)

    # 4. Configurar IA
    modelo = configurar_ia()

    # 4.5. Carregar cache de an√°lises anteriores
    cache = carregar_cache()

    # 5. Processar lojas
    lista_lojas = df_processado[COL_LOJA].unique()
    total_lojas = len(lista_lojas)

    logger.info(f"Iniciando processamento de {total_lojas} lojas...")

    resultado_final = []

    for idx, id_loja in enumerate(lista_lojas, 1):
        logger.info(f"Processando Loja {id_loja} ({idx}/{total_lojas})")

        df_loja = df_processado[df_processado[COL_LOJA] == id_loja]
        resultado_loja = processar_loja(df_loja, id_loja, modelo, cache)
        resultado_final.append(resultado_loja)

    # 6. Salvar cache atualizado
    salvar_cache(cache)
    logger.info("Cache de an√°lises atualizado")

    # 7. Salvar resultado
    if salvar_resultado(resultado_final, ARQUIVO_SAIDA):
        logger.info("=" * 50)
        logger.info("PROCESSAMENTO CONCLU√çDO COM SUCESSO!")
        logger.info(f"Total de lojas processadas: {total_lojas}")
        logger.info(f"Arquivo gerado: {ARQUIVO_SAIDA}")
        logger.info("=" * 50)
    else:
        logger.error("Falha ao salvar resultado final")


if __name__ == "__main__":
    main()